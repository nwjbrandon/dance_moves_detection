{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hapt_quantized.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZfc7Acx3AK"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook evalutes different quantized methods on CNN mdoel to predict the 6 human activities walking, walking upstairs, walking downstairs, sitting, standing or laying from accelerometer and gyroscope readings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4DaZtEQO0co",
        "outputId": "823567ad-c191-4607-c722-ecfa63a0a3fa"
      },
      "source": [
        "!gdown --id 167Jwj7RQF9Ngf-FQOPDWWQ3XCbLdTdNZ\n",
        "!unzip -q uci_har_dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=167Jwj7RQF9Ngf-FQOPDWWQ3XCbLdTdNZ\n",
            "To: /content/uci_har_dataset.zip\n",
            "61.0MB [00:00, 131MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncFN7LJsO_VM"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "random_state = 42\n",
        "\n",
        "activity = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
        "activity_map = {\n",
        "    1: 'WALKING', \n",
        "    2:'WALKING_UPSTAIRS',\n",
        "    3:'WALKING_DOWNSTAIRS',\n",
        "    4:'SITTING', \n",
        "    5:'STANDING',\n",
        "    6:'LAYING'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpz_ZD4TPANz"
      },
      "source": [
        "# load a single file as a numpy array\n",
        "def read_raw_data_feature(filepath):\n",
        "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def read_raw_data_feature_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = read_raw_data_feature(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = np.dstack(loaded)\n",
        "    return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def read_raw_data(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = read_raw_data_feature_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = read_raw_data_feature(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def prepare_raw_data(prefix='uci_har_dataset/'):\n",
        "    # load all train\n",
        "    X_train, y_train = read_raw_data('train', prefix)\n",
        "    # load all test\n",
        "    X_test, y_test = read_raw_data('test', prefix)\n",
        "    # zero-offset class values\n",
        "    y_train = y_train - 1\n",
        "    y_test = y_test - 1\n",
        "\n",
        "    X = np.concatenate([X_train, X_test])\n",
        "    y = np.concatenate([y_train, y_test])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n",
        "    \n",
        "    return X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "X_train, X_valid, X_test, y_train, y_valid, y_test = prepare_raw_data(prefix=\"uci_har_dataset/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ0mwidYPGlc",
        "outputId": "7bc09b87-c3b4-43b3-8a72-56f908b002dc"
      },
      "source": [
        "scaler = preprocessing.StandardScaler()\n",
        "# https://stackoverflow.com/questions/53870113/using-standardscaler-on-3d-data\n",
        "def scale_data(data, is_train=False):\n",
        "    num_instances, num_time_steps, num_features = data.shape\n",
        "    data = np.reshape(data, newshape=(-1, num_features))\n",
        "    if is_train:\n",
        "        data = scaler.fit_transform(data)\n",
        "    else:\n",
        "        data = scaler.transform(data)\n",
        "    data = np.reshape(data, newshape=(num_instances, num_time_steps, num_features))\n",
        "    return data\n",
        "\n",
        "X_train = scale_data(X_train, is_train=True)\n",
        "X_valid = scale_data(X_valid)\n",
        "X_test = scale_data(X_test)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6179, 128, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgJ0J17APKnb"
      },
      "source": [
        "class CNNDataset(object):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        data = self.X[idx]\n",
        "        target = self.y[idx][0]\n",
        "        return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "def prepare_cnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test):\n",
        "    traindataset = CNNDataset(X_train, y_train)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        traindataset, \n",
        "        batch_size=100, \n",
        "        shuffle=True, \n",
        "        num_workers=4,\n",
        "    )\n",
        "\n",
        "    validdataset = CNNDataset(X_valid, y_valid)\n",
        "\n",
        "    validloader = torch.utils.data.DataLoader(\n",
        "        validdataset, \n",
        "        batch_size=100, \n",
        "        shuffle=True, \n",
        "        num_workers=4,\n",
        "    )\n",
        "\n",
        "    testdataset = CNNDataset(X_test, y_test)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testdataset, \n",
        "        batch_size=100, \n",
        "        shuffle=True, \n",
        "        num_workers=4,\n",
        "    )\n",
        "\n",
        "    return trainloader, validloader, testloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAo3m3JuPVGX",
        "outputId": "98daf6b4-cb6f-4bb9-944f-3c9bf0ab3510"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(9, 32, 3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dp1 = nn.Dropout(0.6)\n",
        "\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.flat1 = nn.Flatten()\n",
        "        self.dp2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc1 = nn.Linear(2016, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dp3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dp4 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dp1(x)\n",
        "\n",
        "        x = self.pool1(x)\n",
        "        x = self.flat1(x)\n",
        "        x = self.dp2(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dp3(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.dp4(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "trainloader, validloader, testloader = prepare_cnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test)\n",
        "inputs, labels = next(iter(trainloader))\n",
        "inputs = inputs.permute(0, 2, 1)\n",
        "model = CNN()\n",
        "outputs = model(inputs.float())\n",
        "labels[:1], outputs[:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4]), tensor([[-0.0292, -0.0429, -0.0260,  0.0949, -0.0043,  0.0254]],\n",
              "        grad_fn=<SliceBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OYkdl_9T4Sd"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            上次验证集损失值改善后等待几个epoch\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            如果是True，为每个验证集损失值改善打印一条信息\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            监测数量的最小变化，以符合改进的要求\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''\n",
        "        Saves model when validation loss decrease.\n",
        "        验证损失减少时保存模型。\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix'):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,8))\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cmap='Reds'\n",
        "        print(\"Normalized Confusion Matrix\")\n",
        "    else:\n",
        "        cmap='Greens'\n",
        "        print('Confusion Matrix Without Normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "def plot_avg_loss_per_epoch(avg_train_losses, avg_valid_losses):\n",
        "    # visualize the loss as the network trained\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(avg_train_losses)+1), avg_train_losses, label='Training Loss')\n",
        "    plt.plot(range(1,len(avg_valid_losses)+1), avg_valid_losses,label='Validation Loss')\n",
        "\n",
        "    # find position of lowest validation loss\n",
        "    minposs = avg_train_losses.index(min(avg_train_losses))+1 \n",
        "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.ylim(0, 2) # consistent scale\n",
        "    plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy_per_epoch(train_accuracies, valid_accuracies):\n",
        "    # visualize the loss as the network trained\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(train_accuracies)+1), train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(range(1,len(valid_accuracies)+1), valid_accuracies,label='Valid Accuracy')\n",
        "\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.ylim(0, 1) # consistent scale\n",
        "    plt.xlim(0, len(train_accuracies)+1) # consistent scale\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8unZLflXPYXK"
      },
      "source": [
        "class QCNN(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QCNN, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqHh2mKDPqrZ"
      },
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "def prepare_dataloader():\n",
        "\n",
        "    trainloader, validloader, testloader = prepare_cnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test)\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.permute(0, 2, 1)\n",
        "        outputs = model(inputs.float())\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=10, patience=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    model.to(device)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            inputs = inputs.permute(0, 2, 1)\n",
        "            outputs = model(inputs.float())\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "        \n",
        "        early_stopping(eval_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            break\n",
        "    return model\n",
        "\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.permute(0, 2, 1)\n",
        "        _ = model(inputs.float())\n",
        "\n",
        "def measure_inference_latency(model, device, input_size=(1,3,32,32), num_samples=100):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "    x = x.permute(0, 2, 1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_samples):\n",
        "        _ = model(x.float())\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave\n",
        "\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model():\n",
        "\n",
        "    return CNN()\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        _ = model(x.float())        \n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anRvGyrzzzew"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW5Zo89gQR-u",
        "outputId": "d8eec784-20c5-4624-84b7-96258ae39184"
      },
      "source": [
        "random_seed = 0\n",
        "num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"hapt_model.pt\"\n",
        "quantized_model_filename = \"hapt_model_quantized.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model()\n",
        "\n",
        "train_loader, valid_loader, test_loader = prepare_dataloader()\n",
        "\n",
        "# Train model.\n",
        "print(\"Training Model...\")\n",
        "model = train_model(model=model, train_loader=train_loader, test_loader=valid_loader, device=cuda_device, learning_rate=1e-1, num_epochs=1000, patience=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model...\n",
            "Epoch: -1 Eval Loss: 1.802 Eval Acc: 0.127\n",
            "Epoch: 000 Train Loss: 1.024 Train Acc: 0.552 Eval Loss: 0.506 Eval Acc: 0.817\n",
            "Epoch: 001 Train Loss: 0.421 Train Acc: 0.828 Eval Loss: 0.345 Eval Acc: 0.893\n",
            "Epoch: 002 Train Loss: 0.304 Train Acc: 0.882 Eval Loss: 0.246 Eval Acc: 0.919\n",
            "Epoch: 003 Train Loss: 0.226 Train Acc: 0.913 Eval Loss: 0.215 Eval Acc: 0.935\n",
            "Epoch: 004 Train Loss: 0.209 Train Acc: 0.916 Eval Loss: 0.196 Eval Acc: 0.943\n",
            "Epoch: 005 Train Loss: 0.180 Train Acc: 0.927 Eval Loss: 0.176 Eval Acc: 0.946\n",
            "Epoch: 006 Train Loss: 0.158 Train Acc: 0.938 Eval Loss: 0.165 Eval Acc: 0.949\n",
            "Epoch: 007 Train Loss: 0.148 Train Acc: 0.939 Eval Loss: 0.159 Eval Acc: 0.946\n",
            "Epoch: 008 Train Loss: 0.146 Train Acc: 0.944 Eval Loss: 0.149 Eval Acc: 0.950\n",
            "Epoch: 009 Train Loss: 0.143 Train Acc: 0.941 Eval Loss: 0.155 Eval Acc: 0.947\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 010 Train Loss: 0.128 Train Acc: 0.946 Eval Loss: 0.144 Eval Acc: 0.949\n",
            "Epoch: 011 Train Loss: 0.129 Train Acc: 0.946 Eval Loss: 0.140 Eval Acc: 0.952\n",
            "Epoch: 012 Train Loss: 0.124 Train Acc: 0.951 Eval Loss: 0.143 Eval Acc: 0.947\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 013 Train Loss: 0.122 Train Acc: 0.950 Eval Loss: 0.139 Eval Acc: 0.951\n",
            "Epoch: 014 Train Loss: 0.122 Train Acc: 0.950 Eval Loss: 0.135 Eval Acc: 0.954\n",
            "Epoch: 015 Train Loss: 0.123 Train Acc: 0.950 Eval Loss: 0.137 Eval Acc: 0.950\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 016 Train Loss: 0.115 Train Acc: 0.952 Eval Loss: 0.137 Eval Acc: 0.953\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 017 Train Loss: 0.117 Train Acc: 0.953 Eval Loss: 0.138 Eval Acc: 0.947\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 018 Train Loss: 0.118 Train Acc: 0.954 Eval Loss: 0.135 Eval Acc: 0.952\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 019 Train Loss: 0.112 Train Acc: 0.955 Eval Loss: 0.127 Eval Acc: 0.952\n",
            "Epoch: 020 Train Loss: 0.115 Train Acc: 0.953 Eval Loss: 0.133 Eval Acc: 0.953\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 021 Train Loss: 0.112 Train Acc: 0.956 Eval Loss: 0.136 Eval Acc: 0.955\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 022 Train Loss: 0.110 Train Acc: 0.956 Eval Loss: 0.124 Eval Acc: 0.951\n",
            "Epoch: 023 Train Loss: 0.105 Train Acc: 0.958 Eval Loss: 0.128 Eval Acc: 0.954\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 024 Train Loss: 0.106 Train Acc: 0.957 Eval Loss: 0.127 Eval Acc: 0.955\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 025 Train Loss: 0.109 Train Acc: 0.955 Eval Loss: 0.127 Eval Acc: 0.950\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 026 Train Loss: 0.106 Train Acc: 0.956 Eval Loss: 0.130 Eval Acc: 0.950\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 027 Train Loss: 0.103 Train Acc: 0.960 Eval Loss: 0.126 Eval Acc: 0.953\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 028 Train Loss: 0.110 Train Acc: 0.954 Eval Loss: 0.129 Eval Acc: 0.950\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 029 Train Loss: 0.109 Train Acc: 0.955 Eval Loss: 0.133 Eval Acc: 0.954\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 030 Train Loss: 0.105 Train Acc: 0.957 Eval Loss: 0.123 Eval Acc: 0.953\n",
            "Epoch: 031 Train Loss: 0.107 Train Acc: 0.956 Eval Loss: 0.120 Eval Acc: 0.956\n",
            "Epoch: 032 Train Loss: 0.106 Train Acc: 0.957 Eval Loss: 0.124 Eval Acc: 0.954\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 033 Train Loss: 0.104 Train Acc: 0.956 Eval Loss: 0.124 Eval Acc: 0.957\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 034 Train Loss: 0.102 Train Acc: 0.958 Eval Loss: 0.129 Eval Acc: 0.952\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 035 Train Loss: 0.105 Train Acc: 0.956 Eval Loss: 0.124 Eval Acc: 0.958\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 036 Train Loss: 0.106 Train Acc: 0.958 Eval Loss: 0.118 Eval Acc: 0.954\n",
            "Epoch: 037 Train Loss: 0.102 Train Acc: 0.961 Eval Loss: 0.121 Eval Acc: 0.955\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 038 Train Loss: 0.101 Train Acc: 0.958 Eval Loss: 0.122 Eval Acc: 0.957\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 039 Train Loss: 0.101 Train Acc: 0.957 Eval Loss: 0.124 Eval Acc: 0.955\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 040 Train Loss: 0.100 Train Acc: 0.960 Eval Loss: 0.124 Eval Acc: 0.952\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 041 Train Loss: 0.100 Train Acc: 0.960 Eval Loss: 0.118 Eval Acc: 0.957\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 042 Train Loss: 0.101 Train Acc: 0.960 Eval Loss: 0.120 Eval Acc: 0.959\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 043 Train Loss: 0.098 Train Acc: 0.961 Eval Loss: 0.125 Eval Acc: 0.950\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 044 Train Loss: 0.102 Train Acc: 0.957 Eval Loss: 0.120 Eval Acc: 0.957\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch: 045 Train Loss: 0.099 Train Acc: 0.958 Eval Loss: 0.120 Eval Acc: 0.957\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch: 046 Train Loss: 0.094 Train Acc: 0.962 Eval Loss: 0.117 Eval Acc: 0.955\n",
            "Epoch: 047 Train Loss: 0.096 Train Acc: 0.959 Eval Loss: 0.141 Eval Acc: 0.949\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 048 Train Loss: 0.101 Train Acc: 0.957 Eval Loss: 0.120 Eval Acc: 0.953\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 049 Train Loss: 0.099 Train Acc: 0.958 Eval Loss: 0.131 Eval Acc: 0.951\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 050 Train Loss: 0.096 Train Acc: 0.960 Eval Loss: 0.117 Eval Acc: 0.959\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 051 Train Loss: 0.098 Train Acc: 0.959 Eval Loss: 0.116 Eval Acc: 0.957\n",
            "Epoch: 052 Train Loss: 0.101 Train Acc: 0.959 Eval Loss: 0.118 Eval Acc: 0.955\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 053 Train Loss: 0.098 Train Acc: 0.958 Eval Loss: 0.116 Eval Acc: 0.959\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 054 Train Loss: 0.096 Train Acc: 0.959 Eval Loss: 0.119 Eval Acc: 0.956\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 055 Train Loss: 0.093 Train Acc: 0.961 Eval Loss: 0.119 Eval Acc: 0.957\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 056 Train Loss: 0.094 Train Acc: 0.960 Eval Loss: 0.117 Eval Acc: 0.959\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 057 Train Loss: 0.096 Train Acc: 0.963 Eval Loss: 0.118 Eval Acc: 0.957\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 058 Train Loss: 0.092 Train Acc: 0.960 Eval Loss: 0.119 Eval Acc: 0.956\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 059 Train Loss: 0.093 Train Acc: 0.963 Eval Loss: 0.120 Eval Acc: 0.951\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch: 060 Train Loss: 0.093 Train Acc: 0.961 Eval Loss: 0.116 Eval Acc: 0.958\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch: 061 Train Loss: 0.089 Train Acc: 0.962 Eval Loss: 0.113 Eval Acc: 0.957\n",
            "Epoch: 062 Train Loss: 0.090 Train Acc: 0.960 Eval Loss: 0.110 Eval Acc: 0.958\n",
            "Epoch: 063 Train Loss: 0.087 Train Acc: 0.963 Eval Loss: 0.113 Eval Acc: 0.959\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 064 Train Loss: 0.089 Train Acc: 0.962 Eval Loss: 0.113 Eval Acc: 0.958\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 065 Train Loss: 0.089 Train Acc: 0.963 Eval Loss: 0.112 Eval Acc: 0.955\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 066 Train Loss: 0.089 Train Acc: 0.961 Eval Loss: 0.112 Eval Acc: 0.957\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 067 Train Loss: 0.090 Train Acc: 0.962 Eval Loss: 0.114 Eval Acc: 0.957\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 068 Train Loss: 0.090 Train Acc: 0.960 Eval Loss: 0.116 Eval Acc: 0.958\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 069 Train Loss: 0.086 Train Acc: 0.964 Eval Loss: 0.112 Eval Acc: 0.955\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 070 Train Loss: 0.090 Train Acc: 0.962 Eval Loss: 0.113 Eval Acc: 0.958\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch: 071 Train Loss: 0.085 Train Acc: 0.964 Eval Loss: 0.109 Eval Acc: 0.959\n",
            "Epoch: 072 Train Loss: 0.085 Train Acc: 0.964 Eval Loss: 0.116 Eval Acc: 0.954\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 073 Train Loss: 0.085 Train Acc: 0.965 Eval Loss: 0.110 Eval Acc: 0.957\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 074 Train Loss: 0.081 Train Acc: 0.966 Eval Loss: 0.113 Eval Acc: 0.958\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 075 Train Loss: 0.084 Train Acc: 0.964 Eval Loss: 0.111 Eval Acc: 0.951\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 076 Train Loss: 0.086 Train Acc: 0.964 Eval Loss: 0.115 Eval Acc: 0.953\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 077 Train Loss: 0.086 Train Acc: 0.964 Eval Loss: 0.106 Eval Acc: 0.958\n",
            "Epoch: 078 Train Loss: 0.084 Train Acc: 0.962 Eval Loss: 0.107 Eval Acc: 0.959\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 079 Train Loss: 0.083 Train Acc: 0.965 Eval Loss: 0.110 Eval Acc: 0.956\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 080 Train Loss: 0.078 Train Acc: 0.967 Eval Loss: 0.109 Eval Acc: 0.962\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 081 Train Loss: 0.078 Train Acc: 0.967 Eval Loss: 0.109 Eval Acc: 0.961\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 082 Train Loss: 0.083 Train Acc: 0.967 Eval Loss: 0.104 Eval Acc: 0.959\n",
            "Epoch: 083 Train Loss: 0.082 Train Acc: 0.966 Eval Loss: 0.113 Eval Acc: 0.960\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 084 Train Loss: 0.084 Train Acc: 0.966 Eval Loss: 0.113 Eval Acc: 0.958\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 085 Train Loss: 0.086 Train Acc: 0.963 Eval Loss: 0.106 Eval Acc: 0.961\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 086 Train Loss: 0.074 Train Acc: 0.971 Eval Loss: 0.100 Eval Acc: 0.963\n",
            "Epoch: 087 Train Loss: 0.078 Train Acc: 0.968 Eval Loss: 0.109 Eval Acc: 0.962\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 088 Train Loss: 0.081 Train Acc: 0.966 Eval Loss: 0.105 Eval Acc: 0.961\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 089 Train Loss: 0.077 Train Acc: 0.967 Eval Loss: 0.104 Eval Acc: 0.960\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 090 Train Loss: 0.074 Train Acc: 0.971 Eval Loss: 0.105 Eval Acc: 0.961\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 091 Train Loss: 0.079 Train Acc: 0.968 Eval Loss: 0.106 Eval Acc: 0.961\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 092 Train Loss: 0.076 Train Acc: 0.970 Eval Loss: 0.123 Eval Acc: 0.955\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 093 Train Loss: 0.080 Train Acc: 0.967 Eval Loss: 0.112 Eval Acc: 0.960\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 094 Train Loss: 0.070 Train Acc: 0.969 Eval Loss: 0.103 Eval Acc: 0.960\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch: 095 Train Loss: 0.077 Train Acc: 0.967 Eval Loss: 0.107 Eval Acc: 0.959\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch: 096 Train Loss: 0.074 Train Acc: 0.970 Eval Loss: 0.108 Eval Acc: 0.963\n",
            "EarlyStopping counter: 10 out of 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y0KGdj-ylpt"
      },
      "source": [
        "# Quantized Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpRDXGe1QofS",
        "outputId": "d303af85-23f8-4910-cbc8-051e6f65cb9f"
      },
      "source": [
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.train()\n",
        "fused_model.train()\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model,\n",
        "    [[\"conv1\", \"relu1\"], [\"fc1\", \"relu2\"]])\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "inputs, labels = next(iter(valid_loader))\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=inputs.shape), \"Fused model is not equivalent to the original model!\"\n",
        "\n",
        "# Prepare the model for quantization aware training. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "qatmodel = QCNN(model_fp32=fused_model)\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "qatmodel.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(qatmodel.qconfig)\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "torch.quantization.prepare_qat(qatmodel, inplace=True)\n",
        "\n",
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "qatmodel.train()\n",
        "train_model(model=qatmodel, train_loader=train_loader, test_loader=valid_loader, device=cuda_device, learning_rate=1e-3, num_epochs=1000, patience=10)\n",
        "qatmodel.to(cpu_device)\n",
        "\n",
        "qatmodel = torch.quantization.convert(qatmodel, inplace=True)\n",
        "\n",
        "qatmodel.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(qatmodel)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=qatmodel, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "inputs, labels = next(iter(train_loader))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=qatmodel, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=inputs.shape, num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,))\n",
            "  (relu1): ReLU()\n",
            "  (dp1): Dropout(p=0.6, inplace=False)\n",
            "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flat1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=2016, out_features=256, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (dp4): Dropout(p=0.2, inplace=False)\n",
            "  (fc3): Linear(in_features=128, out_features=6, bias=True)\n",
            ")\n",
            "CNN(\n",
            "  (conv1): ConvReLU1d(\n",
            "    (0): Conv1d(9, 32, kernel_size=(3,), stride=(1,))\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (relu1): Identity()\n",
            "  (dp1): Dropout(p=0.6, inplace=False)\n",
            "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flat1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): LinearReLU(\n",
            "    (0): Linear(in_features=2016, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (relu2): Identity()\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (dp4): Dropout(p=0.2, inplace=False)\n",
            "  (fc3): Linear(in_features=128, out_features=6, bias=True)\n",
            ")\n",
            "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
            "Training QAT Model...\n",
            "Epoch: -1 Eval Loss: 0.108 Eval Acc: 0.963\n",
            "Epoch: 000 Train Loss: 0.073 Train Acc: 0.971 Eval Loss: 0.103 Eval Acc: 0.963\n",
            "Epoch: 001 Train Loss: 0.069 Train Acc: 0.973 Eval Loss: 0.115 Eval Acc: 0.955\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 002 Train Loss: 0.079 Train Acc: 0.969 Eval Loss: 0.105 Eval Acc: 0.960\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 003 Train Loss: 0.078 Train Acc: 0.970 Eval Loss: 0.104 Eval Acc: 0.964\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 004 Train Loss: 0.077 Train Acc: 0.969 Eval Loss: 0.104 Eval Acc: 0.965\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 005 Train Loss: 0.075 Train Acc: 0.969 Eval Loss: 0.101 Eval Acc: 0.960\n",
            "Epoch: 006 Train Loss: 0.073 Train Acc: 0.970 Eval Loss: 0.107 Eval Acc: 0.959\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 007 Train Loss: 0.066 Train Acc: 0.971 Eval Loss: 0.099 Eval Acc: 0.964\n",
            "Epoch: 008 Train Loss: 0.069 Train Acc: 0.971 Eval Loss: 0.106 Eval Acc: 0.957\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 009 Train Loss: 0.069 Train Acc: 0.971 Eval Loss: 0.100 Eval Acc: 0.962\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 010 Train Loss: 0.068 Train Acc: 0.971 Eval Loss: 0.099 Eval Acc: 0.963\n",
            "Epoch: 011 Train Loss: 0.073 Train Acc: 0.970 Eval Loss: 0.105 Eval Acc: 0.963\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 012 Train Loss: 0.070 Train Acc: 0.973 Eval Loss: 0.098 Eval Acc: 0.965\n",
            "Epoch: 013 Train Loss: 0.071 Train Acc: 0.973 Eval Loss: 0.103 Eval Acc: 0.958\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 014 Train Loss: 0.072 Train Acc: 0.972 Eval Loss: 0.100 Eval Acc: 0.964\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 015 Train Loss: 0.067 Train Acc: 0.971 Eval Loss: 0.099 Eval Acc: 0.964\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 016 Train Loss: 0.067 Train Acc: 0.971 Eval Loss: 0.096 Eval Acc: 0.967\n",
            "Epoch: 017 Train Loss: 0.072 Train Acc: 0.972 Eval Loss: 0.100 Eval Acc: 0.964\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 018 Train Loss: 0.064 Train Acc: 0.970 Eval Loss: 0.098 Eval Acc: 0.961\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 019 Train Loss: 0.064 Train Acc: 0.973 Eval Loss: 0.094 Eval Acc: 0.967\n",
            "Epoch: 020 Train Loss: 0.064 Train Acc: 0.974 Eval Loss: 0.095 Eval Acc: 0.965\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch: 021 Train Loss: 0.063 Train Acc: 0.973 Eval Loss: 0.099 Eval Acc: 0.962\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch: 022 Train Loss: 0.072 Train Acc: 0.971 Eval Loss: 0.100 Eval Acc: 0.967\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch: 023 Train Loss: 0.068 Train Acc: 0.972 Eval Loss: 0.099 Eval Acc: 0.962\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch: 024 Train Loss: 0.067 Train Acc: 0.972 Eval Loss: 0.099 Eval Acc: 0.963\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch: 025 Train Loss: 0.068 Train Acc: 0.972 Eval Loss: 0.099 Eval Acc: 0.963\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch: 026 Train Loss: 0.062 Train Acc: 0.974 Eval Loss: 0.097 Eval Acc: 0.965\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch: 027 Train Loss: 0.069 Train Acc: 0.971 Eval Loss: 0.107 Eval Acc: 0.955\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch: 028 Train Loss: 0.062 Train Acc: 0.974 Eval Loss: 0.109 Eval Acc: 0.957\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch: 029 Train Loss: 0.069 Train Acc: 0.974 Eval Loss: 0.105 Eval Acc: 0.960\n",
            "EarlyStopping counter: 10 out of 10\n",
            "QCNN(\n",
            "  (quant): Quantize(scale=tensor([0.1374]), zero_point=tensor([64]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): CNN(\n",
            "    (conv1): QuantizedConvReLU1d(9, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.059679530560970306, zero_point=0)\n",
            "    (relu1): Identity()\n",
            "    (dp1): Dropout(p=0.6, inplace=False)\n",
            "    (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (dp2): Dropout(p=0.2, inplace=False)\n",
            "    (fc1): QuantizedLinearReLU(in_features=2016, out_features=256, scale=0.21038496494293213, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "    (relu2): Identity()\n",
            "    (dp3): Dropout(p=0.2, inplace=False)\n",
            "    (fc2): QuantizedLinear(in_features=256, out_features=128, scale=0.4903569221496582, zero_point=63, qscheme=torch.per_channel_affine)\n",
            "    (dp4): Dropout(p=0.2, inplace=False)\n",
            "    (fc3): QuantizedLinear(in_features=128, out_features=6, scale=2.923888683319092, zero_point=66, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "FP32 evaluation accuracy: 0.964\n",
            "INT8 evaluation accuracy: 0.943\n",
            "FP32 CPU Inference Latency: 6.06 ms / sample\n",
            "FP32 CUDA Inference Latency: 0.44 ms / sample\n",
            "INT8 CPU Inference Latency: 3.69 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 3.09 ms / sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5vIJtLoyxqY"
      },
      "source": [
        "# Static Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKSLTXrmRj1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d81fcea-2441-46dc-9165-2bee9e3b0924"
      },
      "source": [
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.cpu().eval()\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"relu1\"], [\"fc1\", \"relu2\"]], inplace=True)\n",
        "sqmodel = QCNN(model_fp32=fused_model)\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "sqmodel.qconfig = quantization_config\n",
        "torch.quantization.prepare(sqmodel, inplace=True)\n",
        "calibrate_model(model=sqmodel, loader=trainloader)\n",
        "sqmodel = torch.quantization.convert(sqmodel, inplace=True)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=sqmodel, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "inputs, labels = next(iter(train_loader))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=sqmodel, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=inputs.shape, num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FP32 evaluation accuracy: 0.964\n",
            "INT8 evaluation accuracy: 0.950\n",
            "FP32 CPU Inference Latency: 6.02 ms / sample\n",
            "FP32 CUDA Inference Latency: 0.43 ms / sample\n",
            "INT8 CPU Inference Latency: 3.47 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 3.11 ms / sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oNQ_vw6y4vT"
      },
      "source": [
        "# Dynamic Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXpbYVimTIYP",
        "outputId": "6c15133e-7bf3-4f39-bd7a-65150c4cc3bd"
      },
      "source": [
        "model.cpu().eval()\n",
        "dqmodel = torch.quantization.quantize_dynamic(\n",
        "    model,  # the original model\n",
        "    {nn.Conv1d, nn.ReLU, nn.Dropout, nn.MaxPool1d, nn.Flatten, nn.Linear, nn.Softmax},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=dqmodel, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "inputs, labels = next(iter(train_loader))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=dqmodel, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=inputs.shape, num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=inputs.shape, num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FP32 evaluation accuracy: 0.964\n",
            "INT8 evaluation accuracy: 0.967\n",
            "FP32 CPU Inference Latency: 6.16 ms / sample\n",
            "FP32 CUDA Inference Latency: 0.44 ms / sample\n",
            "INT8 CPU Inference Latency: 5.53 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 5.27 ms / sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKAjiVE4y8-b"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnzd1QRPy-Mp"
      },
      "source": [
        "All three quantizated model acheive comparatively high accuracies that is less by at most 1%. The quantized awared trained model achieve the shortest inference latency. The dynamic quantization method could only quantized Linear and LSTM layers only, and therefore, do not have much difference in latency and accuracy as compared to the original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC-QzjaM6Ze2",
        "outputId": "6a3f0186-46b1-4407-8a26-6fa076260723"
      },
      "source": [
        "qatmodel.model_fp32.fc1.weight()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0035,  0.0102,  0.0266,  ..., -0.0270, -0.0111, -0.0385],\n",
              "        [ 0.0192, -0.0310, -0.0015,  ..., -0.0202,  0.0005, -0.0172],\n",
              "        [ 0.0050,  0.0131, -0.0007,  ...,  0.0071,  0.0050,  0.0014],\n",
              "        ...,\n",
              "        [ 0.0094,  0.0228,  0.0239,  ...,  0.0108, -0.0325,  0.0064],\n",
              "        [ 0.0146,  0.0120,  0.0204,  ..., -0.0333,  0.0093, -0.0315],\n",
              "        [ 0.0178,  0.0052,  0.0232,  ...,  0.0087,  0.0211,  0.0054]],\n",
              "       size=(256, 2016), dtype=torch.qint8,\n",
              "       quantization_scheme=torch.per_channel_affine,\n",
              "       scale=tensor([0.0004, 0.0005, 0.0004, 0.0006, 0.0005, 0.0005, 0.0005, 0.0003, 0.0005,\n",
              "        0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0006, 0.0003, 0.0002,\n",
              "        0.0005, 0.0004, 0.0004, 0.0006, 0.0004, 0.0003, 0.0005, 0.0004, 0.0004,\n",
              "        0.0005, 0.0003, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0004, 0.0005,\n",
              "        0.0003, 0.0004, 0.0004, 0.0004, 0.0006, 0.0004, 0.0003, 0.0004, 0.0005,\n",
              "        0.0006, 0.0004, 0.0004, 0.0007, 0.0007, 0.0004, 0.0003, 0.0004, 0.0004,\n",
              "        0.0002, 0.0004, 0.0004, 0.0004, 0.0003, 0.0004, 0.0005, 0.0003, 0.0004,\n",
              "        0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0005, 0.0005, 0.0004, 0.0005,\n",
              "        0.0004, 0.0004, 0.0006, 0.0005, 0.0005, 0.0004, 0.0003, 0.0005, 0.0005,\n",
              "        0.0004, 0.0004, 0.0006, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0002,\n",
              "        0.0003, 0.0004, 0.0002, 0.0004, 0.0005, 0.0003, 0.0003, 0.0004, 0.0004,\n",
              "        0.0003, 0.0004, 0.0003, 0.0004, 0.0004, 0.0005, 0.0003, 0.0003, 0.0005,\n",
              "        0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0005, 0.0003, 0.0004, 0.0005,\n",
              "        0.0004, 0.0003, 0.0003, 0.0005, 0.0003, 0.0003, 0.0005, 0.0004, 0.0005,\n",
              "        0.0006, 0.0004, 0.0004, 0.0003, 0.0006, 0.0003, 0.0003, 0.0004, 0.0003,\n",
              "        0.0006, 0.0005, 0.0005, 0.0005, 0.0004, 0.0003, 0.0004, 0.0006, 0.0005,\n",
              "        0.0004, 0.0003, 0.0003, 0.0003, 0.0005, 0.0004, 0.0005, 0.0006, 0.0006,\n",
              "        0.0004, 0.0003, 0.0003, 0.0004, 0.0003, 0.0005, 0.0004, 0.0006, 0.0003,\n",
              "        0.0004, 0.0005, 0.0007, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0006,\n",
              "        0.0007, 0.0004, 0.0006, 0.0004, 0.0005, 0.0003, 0.0004, 0.0004, 0.0004,\n",
              "        0.0006, 0.0007, 0.0005, 0.0006, 0.0004, 0.0007, 0.0006, 0.0005, 0.0004,\n",
              "        0.0005, 0.0005, 0.0004, 0.0003, 0.0003, 0.0005, 0.0004, 0.0004, 0.0004,\n",
              "        0.0005, 0.0004, 0.0004, 0.0004, 0.0006, 0.0005, 0.0006, 0.0006, 0.0004,\n",
              "        0.0005, 0.0003, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005,\n",
              "        0.0006, 0.0006, 0.0003, 0.0002, 0.0005, 0.0006, 0.0005, 0.0003, 0.0004,\n",
              "        0.0003, 0.0006, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0004,\n",
              "        0.0004, 0.0004, 0.0004, 0.0006, 0.0004, 0.0002, 0.0004, 0.0004, 0.0004,\n",
              "        0.0007, 0.0004, 0.0005, 0.0005, 0.0003, 0.0003, 0.0004, 0.0004, 0.0004,\n",
              "        0.0003, 0.0004, 0.0004, 0.0002], dtype=torch.float64),\n",
              "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "       axis=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK3VIT-nVor_",
        "outputId": "e5274ee7-172b-4050-b174-ef4d937cf1cf"
      },
      "source": [
        "qatmodel.model_fp32.fc1.weight().int_repr()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  8,  23,  60,  ..., -61, -25, -87],\n",
              "        [ 39, -63,  -3,  ..., -41,   1, -35],\n",
              "        [ 14,  37,  -2,  ...,  20,  14,   4],\n",
              "        ...,\n",
              "        [ 25,  61,  64,  ...,  29, -87,  17],\n",
              "        [ 33,  27,  46,  ..., -75,  21, -71],\n",
              "        [ 76,  22,  99,  ...,  37,  90,  23]], dtype=torch.int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}